{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "XzfPv33GFDEM",
        "XJvd6IgCWo6Z",
        "NtByu6gKlrOd",
        "WYCe7GxJpwJW"
      ],
      "authorship_tag": "ABX9TyNzjFIoMgnu4hzZ+R+DtoIA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubha07m/LLM_Dialogue_Generation/blob/main/tuned_llm_benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and loading library"
      ],
      "metadata": {
        "id": "0s056dEhZWOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install datasets\n",
        "! pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bsIk823sAZo",
        "outputId": "033882a2-8d4e-48fd-b38a-25334493c71b",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3LNogZpAeMM",
        "outputId": "6183d27c-7b7f-4de6-f7a5-96e94c24cd34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importing library and drive\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn.functional import cross_entropy\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the performance of new tuned LLM"
      ],
      "metadata": {
        "id": "XzfPv33GFDEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the model and tokenizer in Google Drive\n",
        "model_path = '/content/drive/MyDrive/blenderbot_llm'\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Ensure decoder_start_token_id is set in the model\n",
        "model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids('<s>')\n"
      ],
      "metadata": {
        "id": "y5SzwBmKA-vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode input\n",
        "inputs = tokenizer(\"Lex Fridman: What is the future of AI?\\nLee Cronin:\", return_tensors='pt')\n",
        "\n",
        "# Generate output with proper handling of special tokens\n",
        "outputs = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=60,\n",
        "    num_beams=10,  # Increase number of beams for diversity\n",
        "    length_penalty=1.0,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,  # Increase top-p for diversity\n",
        "    top_k=50,  # Use top-k sampling\n",
        "    temperature=1.5,  # Increase temperature for more randomness\n",
        "    early_stopping=True,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=model.config.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Output:\", decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgVEgedPSrs6",
        "outputId": "033d09f2-3c7b-477f-8a51-cd4654f4f4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Output:  lee cronin:  what do you do for a living?  i work as a graphic designer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using advanced prompt engineering for conversation generation"
      ],
      "metadata": {
        "id": "XJvd6IgCWo6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation(prompt, model, tokenizer, max_turns=12, max_length=60, max_input_length=128):\n",
        "    conversation = prompt\n",
        "    current_speaker = \"Lee Cronin\"\n",
        "    for _ in range(max_turns):\n",
        "        # Encode input with truncation\n",
        "        inputs = tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)\n",
        "\n",
        "        # Generate output\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=max_length,\n",
        "            num_beams=10,\n",
        "            length_penalty=1.0,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            temperature=1.5,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=model.config.decoder_start_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=model.config.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the output and clean up the response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_text = generated_text.replace(current_speaker.lower(), \"\").replace(current_speaker, \"\").strip().split('. ')[0] + '.'\n",
        "\n",
        "        # Update conversation with new speaker\n",
        "        conversation += f\"\\n{current_speaker}: {generated_text}\"\n",
        "\n",
        "        # Alternate speaker\n",
        "        current_speaker = \"Lex Fridman\" if current_speaker == \"Lee Cronin\" else \"Lee Cronin\"\n",
        "\n",
        "        # Ensure conversation does not exceed max length\n",
        "        if tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'].shape[1] > max_input_length:\n",
        "            conversation = tokenizer.decode(tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'][0, -max_input_length:])\n",
        "\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "1CjtmYotdpeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained('/content/drive/MyDrive/blenderbot_llm')\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained('/content/drive/MyDrive/blenderbot_llm')\n",
        "\n",
        "# Define initial prompt with one sentence from each speaker\n",
        "initial_prompt = \"\"\"Lex Fridman: What is the future of AI?\n",
        "Lee Cronin: It’s a fascinating question, and it touches on many aspects of science and technology.\"\"\"\n",
        "\n",
        "# Generate conversation\n",
        "conversation = generate_conversation(initial_prompt, model, tokenizer)\n",
        "print(\"Generated Conversation:\\n\", conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rutJ8QmbfUga",
        "outputId": "aed35e60-fc4b-48da-d032-ee3ff7f78fd4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Conversation:\n",
            " Lex Fridman: What is the future of AI?\n",
            "Lee Cronin: It’s a fascinating question, and it touches on many aspects of science and technology.\n",
            "Lee Cronin: :  i think it is fascinating that we have the ability to look at the past and think about the future.\n",
            "Lex Fridman: I think the future is going to look very different than the past because in the future we will have to look for the future in the present and then we will look back in the past to see what it will look like.\n",
            "Lee Cronin: That is a very interesting way of looking at it.\n",
            "Lex Fridman: Listening to the soundtrack on my smartphone right now.\n",
            "Lee Cronin: Listening to some of my favorite musicians right now, one of my favorites is Led Zeppelin, what about you?.\n",
            "Lex Fridman: Listening to some of my favorite music from the past right now is the Beatles.\n",
            "Lee Cronin: Lets see what the future has in store for us.\n",
            "Lex Fridman: Lets look forward to the future and look at how the future will look and what will it look like and what do you think about it?.\n",
            "Lee Cronin: Listening to music by James Fenimore Cooper is one of my favorite things to do in the mornings..\n",
            "Lex Fridman: Hello, how are you today? Do you have any hobbies? I like to go to museums and look at artifacts..\n",
            "Lee Cronin: Listening to some of my favorite musicians right now, one of my favorites is Frank Sinatra.\n",
            "Lex Fridman: Lets take a step back and talk about the past so that we can look forward to the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the performance of EWC tuned LLM on new data"
      ],
      "metadata": {
        "id": "NtByu6gKlrOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the model and tokenizer in Google Drive\n",
        "model_path = '/content/drive/MyDrive/ewc_trained_llm'\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Ensure decoder_start_token_id is set in the model\n",
        "model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids('<s>')\n"
      ],
      "metadata": {
        "id": "LGVDgOPJlwpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation(prompt, model, tokenizer, max_turns=12, max_length=60, max_input_length=128):\n",
        "    conversation = prompt\n",
        "    current_speaker = \"Lisa Randall\"\n",
        "\n",
        "    for _ in range(max_turns):\n",
        "        # Encode input with truncation\n",
        "        inputs = tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)\n",
        "\n",
        "        # Generate output\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'].to(model.device),\n",
        "            max_length=max_length,\n",
        "            num_beams=10,\n",
        "            length_penalty=1.0,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            temperature=1.5,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=model.config.decoder_start_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=model.config.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the output and clean up the response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_text = generated_text.replace(current_speaker.lower(), \"\").replace(current_speaker, \"\").strip().split('. ')[0] + '.'\n",
        "\n",
        "        # Update conversation with new speaker\n",
        "        conversation += f\"\\n{current_speaker}: {generated_text}\"\n",
        "\n",
        "        # Alternate speaker\n",
        "        current_speaker = \"Lex Fridman\" if current_speaker == \"Lisa Randall\" else \"Lisa Randall\"\n",
        "\n",
        "        # Ensure conversation does not exceed max length\n",
        "        if tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'].shape[1] > max_input_length:\n",
        "            conversation = tokenizer.decode(tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'][0, -max_input_length:])\n",
        "\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "Qu_S0WV4ozkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the initial prompt\n",
        "initial_prompt = \"\"\"Lex Fridman: How do you envision the future of our understanding of the universe?\n",
        "Lisa Randall: The future of our understanding lies in the mysteries we have yet to uncover, particularly with dark matter and other cosmic phenomena.\"\"\"\n",
        "\n",
        "# Generate conversation\n",
        "generated_conversation = generate_conversation(initial_prompt, model, tokenizer)\n",
        "print(generated_conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6gKvIx5mP6r",
        "outputId": "cc2f0799-59e5-40f6-b0f8-9ef38936ac05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lex Fridman: How do you envision the future of our understanding of the universe?\n",
            "Lisa Randall: The future of our understanding lies in the mysteries we have yet to uncover, particularly with dark matter and other cosmic phenomena.\n",
            "Lisa Randall: the to is you and that.\n",
            "Lex Fridman: the is that of a it to.\n",
            "Lisa Randall: is the to i you so that.\n",
            "Lex Fridman: the are you is i of.\n",
            "Lisa Randall: is that the to and it.\n",
            "Lex Fridman: the of and a have is.\n",
            "Lisa Randall: that the is in and life i.\n",
            "Lex Fridman: and is the that i it of.\n",
            "Lisa Randall: the a and that is in.\n",
            "Lex Fridman: the you i to is that and.\n",
            "Lisa Randall: it is the to i and.\n",
            "Lex Fridman: that is you the in so.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading both the old and new model and tokenizer for testing"
      ],
      "metadata": {
        "id": "5YIFoT4eNERU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "old_model_path = '/content/drive/MyDrive/blenderbot_llm'\n",
        "new_model_path = '/content/drive/MyDrive/ewc_trained_llm'\n",
        "\n",
        "# Load the tokenizers\n",
        "tokenizer_old = BlenderbotTokenizer.from_pretrained(old_model_path)\n",
        "tokenizer_new = BlenderbotTokenizer.from_pretrained(new_model_path)\n",
        "\n",
        "# Load the models\n",
        "model_old = BlenderbotForConditionalGeneration.from_pretrained(old_model_path)\n",
        "model_new = BlenderbotForConditionalGeneration.from_pretrained(new_model_path)\n",
        "\n",
        "# Ensure the models are on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_old.to(device)\n",
        "model_new.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuypKSA6p1i9",
        "outputId": "d2f7820d-5eda-467e-8ffa-700559f7a4fb",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BlenderbotForConditionalGeneration(\n",
              "  (model): BlenderbotModel(\n",
              "    (shared): BlenderbotScaledWordEmbedding(8008, 1280, padding_idx=0)\n",
              "    (encoder): BlenderbotEncoder(\n",
              "      (embed_tokens): BlenderbotScaledWordEmbedding(8008, 1280, padding_idx=0)\n",
              "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x BlenderbotEncoderLayer(\n",
              "          (self_attn): BlenderbotAttention(\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BlenderbotDecoder(\n",
              "      (embed_tokens): BlenderbotScaledWordEmbedding(8008, 1280, padding_idx=0)\n",
              "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BlenderbotDecoderLayer(\n",
              "          (self_attn): BlenderbotAttention(\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BlenderbotAttention(\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1280, out_features=8008, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating test data from the available conversation"
      ],
      "metadata": {
        "id": "1lIgi8-5VkHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read files directly from Google Drive\n",
        "\n",
        "def download_and_read_file_from_drive(file_id, file_name):\n",
        "    \"\"\"\n",
        "    Downloads a file from Google Drive using the file ID and reads it into a Pandas DataFrame if it's a CSV file.\n",
        "\n",
        "    Args:\n",
        "    file_id (str): The ID of the file in Google Drive.\n",
        "    file_name (str): The name to save the file as (including extension).\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A Pandas DataFrame if the file is a CSV file, otherwise None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        # Authenticate and create the PyDrive client\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "\n",
        "        # Create a GoogleDriveFile instance with the file ID\n",
        "        downloaded = drive.CreateFile({'id': file_id})\n",
        "        downloaded.GetContentFile(file_name)\n",
        "\n",
        "        print(f'File {file_name} downloaded successfully.')\n",
        "\n",
        "        # Check if the file is a CSV file and read it into a DataFrame\n",
        "        if file_name.endswith('.csv'):\n",
        "            df = pd.read_csv(file_name)\n",
        "            print('CSV file read into DataFrame.')\n",
        "            return df\n",
        "        else:\n",
        "            print('File is not a CSV. No DataFrame created.')\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'An error occurred: {e}')\n",
        "        return None"
      ],
      "metadata": {
        "id": "VQvoRKsWV4el"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading first file\n",
        "\n",
        "file_id = '18g5y5GmBQNgU8z2fPdushrdu0XfmQjph'\n",
        "file_name = 'lee_cronin3.csv'\n",
        "\n",
        "# file_id = '15EGbylkuobQtA0zXkeHmmhhNaIxoz50D'\n",
        "# file_name = 'lee_cronin3.csv'\n",
        "df1 = download_and_read_file_from_drive(file_id, file_name)\n",
        "\n",
        "# Load Data\n",
        "initial_data = df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRfhxSkaWACp",
        "outputId": "e44fd7f8-1ddb-400c-c669-946ce0b8cba2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File lee_cronin3.csv downloaded successfully.\n",
            "CSV file read into DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the second file\n",
        "\n",
        "file_id = '1Rm-ItCDv44iDqLaaEZTz-Cqu_xQPM6s5'\n",
        "file_name = 'lisa_randall.csv'\n",
        "\n",
        "# file_id = '1x3prg2ZD8h4PfOkd3Ftohyy8gtDPR3-v'\n",
        "# file_name = 'lisa_randall.csv'\n",
        "df2 = download_and_read_file_from_drive(file_id, file_name)\n",
        "new_data =df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bOY1Dq6XkIz",
        "outputId": "c6e856dc-a42f-4927-8633-7937f09e87a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File lisa_randall.csv downloaded successfully.\n",
            "CSV file read into DataFrame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for creating test data\n",
        "def create_test_data(initial_data, new_data):\n",
        "    def format_data(data):\n",
        "        formatted_data = []\n",
        "        for i in range(0, len(data) - 1, 2):\n",
        "            # Ensure the data is valid\n",
        "            if data.loc[i, 'speaker'] != data.loc[i + 1, 'speaker']:\n",
        "                formatted_data.append({\n",
        "                    'input': f\"{data.loc[i, 'speaker']}: {data.loc[i, 'text']}\",\n",
        "                    'output': f\"{data.loc[i + 1, 'speaker']}: {data.loc[i + 1, 'text']}\"\n",
        "                })\n",
        "        return formatted_data\n",
        "\n",
        "    # Create test data\n",
        "    initial_test_data = format_data(initial_data)\n",
        "    new_test_data = format_data(new_data)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_initial_test = pd.DataFrame(initial_test_data)\n",
        "    df_new_test = pd.DataFrame(new_test_data)\n",
        "\n",
        "    # Validate the format of the DataFrames\n",
        "    def validate_dataframe(df):\n",
        "        if 'input' not in df.columns or 'output' not in df.columns:\n",
        "            raise ValueError(\"DataFrame must contain 'input' and 'output' columns.\")\n",
        "        if df.empty:\n",
        "            raise ValueError(\"DataFrame is empty. Ensure data is properly formatted.\")\n",
        "        for index, row in df.iterrows():\n",
        "            if not isinstance(row['input'], str) or not isinstance(row['output'], str):\n",
        "                raise ValueError(f\"Invalid data at index {index}: {row}\")\n",
        "\n",
        "    # Validate DataFrames\n",
        "    validate_dataframe(df_initial_test)\n",
        "    validate_dataframe(df_new_test)\n",
        "\n",
        "    return df_initial_test, df_new_test"
      ],
      "metadata": {
        "id": "rIDV5cVuWFFH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "df_initial_test, df_new_test = create_test_data(initial_data, new_data)"
      ],
      "metadata": {
        "id": "rV2Fl-EGgEMx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df_initial_test.shape, len(df_initial_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iOYjJLGgJwL",
        "outputId": "7ccd381d-8aff-4f07-ddd3-58d5037b1dcb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((335, 2), 335)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial Test Data:\")\n",
        "print(df_initial_test.head())\n",
        "\n",
        "print(\"New Test Data:\")\n",
        "print(df_new_test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sF42MNrBXPC-",
        "outputId": "700529c2-010e-40ae-e0d4-c07d97fa76fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Test Data:\n",
            "                                               input  \\\n",
            "0  lee cronin:  every star in the sky probably ha...   \n",
            "1   lee cronin:  time and the ability to communicate   \n",
            "2  lee cronin:  yeah my biggest fear in a way is ...   \n",
            "3                                lee cronin:  thanks   \n",
            "4                             lee cronin:  go for it   \n",
            "\n",
            "                                              output  \n",
            "0  lex fridman:  intersect you dont mean in time ...  \n",
            "1           lex fridman:  the ability to communicate  \n",
            "2  lex fridman:  the following is a conversation ...  \n",
            "3  lex fridman:  it created i think its fair to s...  \n",
            "4  lex fridman:  so assembly theory says that if ...  \n",
            "New Test Data:\n",
            "                                               input  \\\n",
            "0  lex fridman:  in theory it behaves just like a...   \n",
            "1  lisa randall:  theres also just more of it and...   \n",
            "2  lisa randall:  exactly in my book i make jokes...   \n",
            "3  lisa randall:  exactly no but it is a metaphor...   \n",
            "4  lex fridman:  yeah but a lot of our intuition ...   \n",
            "\n",
            "                                              output  \n",
            "0  lisa randall:  when we say it interacts just l...  \n",
            "1  lex fridman:  its part of the story of the ori...  \n",
            "2  lex fridman:  thats a metaphor on top of a met...  \n",
            "3  lex fridman:  yeah but the things we cannot se...  \n",
            "4  lisa randall:  thats absolutely true certainly...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to list of tuples\n",
        "def convert_df_to_tuples(df):\n",
        "    return list(zip(df['input'], df['output']))\n",
        "\n",
        "test_data_initial_tuples = convert_df_to_tuples(df_initial_test)\n",
        "test_data_new_tuples = convert_df_to_tuples(df_new_test)"
      ],
      "metadata": {
        "id": "1s016ykphIB3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare using test data performance of both model"
      ],
      "metadata": {
        "id": "76GponnVJoWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response\n",
        "\n",
        "def generate_response(model, tokenizer, prompt, device='cuda'):\n",
        "    model.to(device)\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Truncate input sequence if it's too long\n",
        "    max_length = model.config.max_position_embeddings\n",
        "    if inputs.size(1) > max_length:\n",
        "        inputs = inputs[:, :max_length]\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=50,\n",
        "        num_beams=5,\n",
        "        early_stopping=True,\n",
        "        decoder_start_token_id=model.config.pad_token_id  # Ensure correct starting token\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "9k0vVhgmMa5a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute BLEU score\n",
        "def compute_bleu(reference, candidate):\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "    return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=SmoothingFunction().method1)"
      ],
      "metadata": {
        "id": "TIM23VMDULYi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(model, tokenizer, text):\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    device = next(model.parameters()).device\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Truncate input sequence if it's too long\n",
        "    max_length = model.config.max_position_embeddings\n",
        "    if input_ids.size(1) > max_length:\n",
        "        input_ids = input_ids[:, :max_length]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        perplexity = np.exp(loss.item())\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "KN-eNABjUQ1Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(test_data, tokenizer_old, model_old, tokenizer_new, model_new):\n",
        "    # Ensure models are on the correct device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model_old.to(device)\n",
        "    model_new.to(device)\n",
        "\n",
        "    bleu_scores_old = []\n",
        "    perplexity_scores_old = []\n",
        "    bleu_scores_new = []\n",
        "    perplexity_scores_new = []\n",
        "\n",
        "    for prompt, reference in test_data:\n",
        "        response_old = generate_response(model_old, tokenizer_old, prompt, device=device)\n",
        "        response_new = generate_response(model_new, tokenizer_new, prompt, device=device)\n",
        "\n",
        "        bleu_old = compute_bleu(reference, response_old)\n",
        "        bleu_new = compute_bleu(reference, response_new)\n",
        "        perplexity_old = compute_perplexity(model_old, tokenizer_old, response_old)\n",
        "        perplexity_new = compute_perplexity(model_new, tokenizer_new, response_new)\n",
        "\n",
        "        bleu_scores_old.append(bleu_old)\n",
        "        bleu_scores_new.append(bleu_new)\n",
        "        perplexity_scores_old.append(perplexity_old)\n",
        "        perplexity_scores_new.append(perplexity_new)\n",
        "\n",
        "    # Average BLEU and perplexity scores\n",
        "    avg_bleu_old = np.mean(bleu_scores_old)\n",
        "    avg_bleu_new = np.mean(bleu_scores_new)\n",
        "    avg_perplexity_old = np.mean(perplexity_scores_old)\n",
        "    avg_perplexity_new = np.mean(perplexity_scores_new)\n",
        "\n",
        "    return avg_bleu_old, avg_perplexity_old, avg_bleu_new, avg_perplexity_new"
      ],
      "metadata": {
        "id": "JcKhwqshUV9_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate models on initial test data\n",
        "avg_bleu_old_initial, avg_perplexity_old_initial, avg_bleu_new_initial, avg_perplexity_new_initial = evaluate_models(\n",
        "    test_data_initial_tuples, tokenizer_old, model_old, tokenizer_new, model_new\n",
        ")\n",
        "\n",
        "print(\"Initial Test Data Evaluation:\")\n",
        "print(f\"Old Model BLEU Score: {avg_bleu_old_initial}\")\n",
        "print(f\"Old Model Perplexity Score: {avg_perplexity_old_initial}\")\n",
        "print(f\"New Model BLEU Score: {avg_bleu_new_initial}\")\n",
        "print(f\"New Model Perplexity Score: {avg_perplexity_new_initial}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqINM4TAUabQ",
        "outputId": "2f002ca1-5f47-4793-f628-909b41a8cfd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (231 > 128). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (231 > 128). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Test Data Evaluation:\n",
            "Old Model BLEU Score: 0.004999871618429517\n",
            "Old Model Perplexity Score: 4.237836855932256\n",
            "New Model BLEU Score: 0.007313616651345576\n",
            "New Model Perplexity Score: 71.33681441645774\n"
          ]
        }
      ]
    }
  ]
}