{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdB9jQlIOuZnp3yEDHfWcY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubha07m/LLM_Dialogue_Generation/blob/main/tuned_llm_benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the performance of new tuned LLM"
      ],
      "metadata": {
        "id": "XzfPv33GFDEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bsIk823sAZo",
        "outputId": "d489aa75-dc8d-409c-d8ad-a6ff611a9ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3LNogZpAeMM",
        "outputId": "45463df5-8879-4aea-d992-cd68b7182a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Importing library and drive\n",
        "\n",
        "from google.colab import drive\n",
        "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the model and tokenizer in Google Drive\n",
        "model_path = '/content/drive/MyDrive/blenderbot_llm'\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Ensure decoder_start_token_id is set in the model\n",
        "model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids('<s>')\n"
      ],
      "metadata": {
        "id": "y5SzwBmKA-vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode input\n",
        "inputs = tokenizer(\"Lex Fridman: What is the future of AI?\\nLee Cronin:\", return_tensors='pt')\n",
        "\n",
        "# Generate output with proper handling of special tokens\n",
        "outputs = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=60,\n",
        "    num_beams=10,  # Increase number of beams for diversity\n",
        "    length_penalty=1.0,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,  # Increase top-p for diversity\n",
        "    top_k=50,  # Use top-k sampling\n",
        "    temperature=1.5,  # Increase temperature for more randomness\n",
        "    early_stopping=True,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=model.config.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Output:\", decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgVEgedPSrs6",
        "outputId": "033d09f2-3c7b-477f-8a51-cd4654f4f4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Output:  lee cronin:  what do you do for a living?  i work as a graphic designer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using advanced prompt engineering for conversation generation"
      ],
      "metadata": {
        "id": "XJvd6IgCWo6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation(prompt, model, tokenizer, max_turns=12, max_length=60, max_input_length=128):\n",
        "    conversation = prompt\n",
        "    current_speaker = \"Lee Cronin\"\n",
        "    for _ in range(max_turns):\n",
        "        # Encode input with truncation\n",
        "        inputs = tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)\n",
        "\n",
        "        # Generate output\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=max_length,\n",
        "            num_beams=10,\n",
        "            length_penalty=1.0,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            temperature=1.5,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=model.config.decoder_start_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=model.config.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the output and clean up the response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_text = generated_text.replace(current_speaker.lower(), \"\").replace(current_speaker, \"\").strip().split('. ')[0] + '.'\n",
        "\n",
        "        # Update conversation with new speaker\n",
        "        conversation += f\"\\n{current_speaker}: {generated_text}\"\n",
        "\n",
        "        # Alternate speaker\n",
        "        current_speaker = \"Lex Fridman\" if current_speaker == \"Lee Cronin\" else \"Lee Cronin\"\n",
        "\n",
        "        # Ensure conversation does not exceed max length\n",
        "        if tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'].shape[1] > max_input_length:\n",
        "            conversation = tokenizer.decode(tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'][0, -max_input_length:])\n",
        "\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "1CjtmYotdpeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained('/content/drive/MyDrive/blenderbot_llm')\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained('/content/drive/MyDrive/blenderbot_llm')\n",
        "\n",
        "# Define initial prompt with one sentence from each speaker\n",
        "initial_prompt = \"\"\"Lex Fridman: What is the future of AI?\n",
        "Lee Cronin: It’s a fascinating question, and it touches on many aspects of science and technology.\"\"\"\n",
        "\n",
        "# Generate conversation\n",
        "conversation = generate_conversation(initial_prompt, model, tokenizer)\n",
        "print(\"Generated Conversation:\\n\", conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rutJ8QmbfUga",
        "outputId": "aed35e60-fc4b-48da-d032-ee3ff7f78fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Conversation:\n",
            " Lex Fridman: What is the future of AI?\n",
            "Lee Cronin: It’s a fascinating question, and it touches on many aspects of science and technology.\n",
            "Lee Cronin: :  i think it is fascinating that we have the ability to look at the past and think about the future.\n",
            "Lex Fridman: I think the future is going to look very different than the past because in the future we will have to look for the future in the present and then we will look back in the past to see what it will look like.\n",
            "Lee Cronin: That is a very interesting way of looking at it.\n",
            "Lex Fridman: Listening to the soundtrack on my smartphone right now.\n",
            "Lee Cronin: Listening to some of my favorite musicians right now, one of my favorites is Led Zeppelin, what about you?.\n",
            "Lex Fridman: Listening to some of my favorite music from the past right now is the Beatles.\n",
            "Lee Cronin: Lets see what the future has in store for us.\n",
            "Lex Fridman: Lets look forward to the future and look at how the future will look and what will it look like and what do you think about it?.\n",
            "Lee Cronin: Listening to music by James Fenimore Cooper is one of my favorite things to do in the mornings..\n",
            "Lex Fridman: Hello, how are you today? Do you have any hobbies? I like to go to museums and look at artifacts..\n",
            "Lee Cronin: Listening to some of my favorite musicians right now, one of my favorites is Frank Sinatra.\n",
            "Lex Fridman: Lets take a step back and talk about the past so that we can look forward to the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the performance of EWC tuned LLM on new data"
      ],
      "metadata": {
        "id": "NtByu6gKlrOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the model and tokenizer in Google Drive\n",
        "model_path = '/content/drive/MyDrive/ewc_trained_llm'\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Ensure decoder_start_token_id is set in the model\n",
        "model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids('<s>')\n"
      ],
      "metadata": {
        "id": "LGVDgOPJlwpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation(prompt, model, tokenizer, max_turns=12, max_length=60, max_input_length=128):\n",
        "    conversation = prompt\n",
        "    current_speaker = \"Lisa Randall\"\n",
        "\n",
        "    for _ in range(max_turns):\n",
        "        # Encode input with truncation\n",
        "        inputs = tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)\n",
        "\n",
        "        # Generate output\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'].to(model.device),\n",
        "            max_length=max_length,\n",
        "            num_beams=10,\n",
        "            length_penalty=1.0,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            temperature=1.5,\n",
        "            early_stopping=True,\n",
        "            decoder_start_token_id=model.config.decoder_start_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=model.config.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the output and clean up the response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_text = generated_text.replace(current_speaker.lower(), \"\").replace(current_speaker, \"\").strip().split('. ')[0] + '.'\n",
        "\n",
        "        # Update conversation with new speaker\n",
        "        conversation += f\"\\n{current_speaker}: {generated_text}\"\n",
        "\n",
        "        # Alternate speaker\n",
        "        current_speaker = \"Lex Fridman\" if current_speaker == \"Lisa Randall\" else \"Lisa Randall\"\n",
        "\n",
        "        # Ensure conversation does not exceed max length\n",
        "        if tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'].shape[1] > max_input_length:\n",
        "            conversation = tokenizer.decode(tokenizer(conversation, return_tensors='pt', truncation=True, max_length=max_input_length)['input_ids'][0, -max_input_length:])\n",
        "\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "Qu_S0WV4ozkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the initial prompt\n",
        "initial_prompt = \"\"\"Lex Fridman: How do you envision the future of our understanding of the universe?\n",
        "Lisa Randall: The future of our understanding lies in the mysteries we have yet to uncover, particularly with dark matter and other cosmic phenomena.\"\"\"\n",
        "\n",
        "# Generate conversation\n",
        "generated_conversation = generate_conversation(initial_prompt, model, tokenizer)\n",
        "print(generated_conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6gKvIx5mP6r",
        "outputId": "cc2f0799-59e5-40f6-b0f8-9ef38936ac05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lex Fridman: How do you envision the future of our understanding of the universe?\n",
            "Lisa Randall: The future of our understanding lies in the mysteries we have yet to uncover, particularly with dark matter and other cosmic phenomena.\n",
            "Lisa Randall: the to is you and that.\n",
            "Lex Fridman: the is that of a it to.\n",
            "Lisa Randall: is the to i you so that.\n",
            "Lex Fridman: the are you is i of.\n",
            "Lisa Randall: is that the to and it.\n",
            "Lex Fridman: the of and a have is.\n",
            "Lisa Randall: that the is in and life i.\n",
            "Lex Fridman: and is the that i it of.\n",
            "Lisa Randall: the a and that is in.\n",
            "Lex Fridman: the you i to is that and.\n",
            "Lisa Randall: it is the to i and.\n",
            "Lex Fridman: that is you the in so.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing forgetfulness with perplexity score"
      ],
      "metadata": {
        "id": "WYCe7GxJpwJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "old_model_path = '/content/drive/MyDrive/blenderbot_llm'\n",
        "new_model_path = '/content/drive/MyDrive/ewc_trained_llm'\n",
        "\n",
        "# Load the tokenizers\n",
        "tokenizer_old = BlenderbotTokenizer.from_pretrained(old_model_path)\n",
        "tokenizer_new = BlenderbotTokenizer.from_pretrained(new_model_path)\n",
        "\n",
        "# Load the models\n",
        "model_old = BlenderbotForConditionalGeneration.from_pretrained(old_model_path)\n",
        "model_new = BlenderbotForConditionalGeneration.from_pretrained(new_model_path)\n",
        "\n",
        "# Ensure the models are on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_old.to(device)\n",
        "model_new.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuypKSA6p1i9",
        "outputId": "8edccabb-5121-47c6-9e7c-f283a9d75742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BlenderbotForConditionalGeneration(\n",
              "  (model): BlenderbotModel(\n",
              "    (shared): BlenderbotScaledWordEmbedding(8008, 1280, padding_idx=0)\n",
              "    (encoder): BlenderbotEncoder(\n",
              "      (embed_tokens): BlenderbotScaledWordEmbedding(8008, 1280, padding_idx=0)\n",
              "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x BlenderbotEncoderLayer(\n",
              "          (self_attn): BlenderbotAttention(\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BlenderbotDecoder(\n",
              "      (embed_tokens): BlenderbotScaledWordEmbedding(8008, 1280, padding_idx=0)\n",
              "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BlenderbotDecoderLayer(\n",
              "          (self_attn): BlenderbotAttention(\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BlenderbotAttention(\n",
              "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1280, out_features=8008, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, tokenizer, dataloader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_words = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = torch.stack(batch['input_ids']).to(device)\n",
        "            attention_mask = torch.stack(batch['attention_mask']).to(device)\n",
        "            labels = torch.stack(batch['labels']).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)  # Multiply loss by batch size\n",
        "            total_words += input_ids.size(0) * input_ids.size(1)  # Number of tokens\n",
        "\n",
        "    # Average loss\n",
        "    average_loss = total_loss / total_words\n",
        "\n",
        "    # Perplexity is the exponentiation of the average loss\n",
        "    perplexity = np.exp(average_loss)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "A25lM-SgqwkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataloaders\n",
        "with open('/content/drive/MyDrive/saved_dataloader_data/old_dataloader.pkl', 'rb') as f:\n",
        "    old_dataloader = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/saved_dataloader_data/new_dataloader.pkl', 'rb') as f:\n",
        "    new_dataloader = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-5vjOlBrTAb",
        "outputId": "f038a925-cd20-4aaa-af5c-87161d384496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming old_dataloader is prepared similarly to new_dataloader\n",
        "\n",
        "# Evaluate on old data\n",
        "perplexity_old_model_on_old_data = calculate_perplexity(model_old, tokenizer_old, old_dataloader, device)\n",
        "print(f\"Perplexity of Old Model on Old Data: {perplexity_old_model_on_old_data:.4f}\")\n",
        "\n",
        "perplexity_new_model_on_old_data = calculate_perplexity(model_new, tokenizer_new, old_dataloader, device)\n",
        "print(f\"Perplexity of New Model on Old Data: {perplexity_new_model_on_old_data:.4f}\")\n",
        "\n",
        "# Evaluate on new data\n",
        "perplexity_old_model_on_new_data = calculate_perplexity(model_old, tokenizer_old, new_dataloader, device)\n",
        "print(f\"Perplexity of Old Model on New Data: {perplexity_old_model_on_new_data:.4f}\")\n",
        "\n",
        "perplexity_new_model_on_new_data = calculate_perplexity(model_new, tokenizer_new, new_dataloader, device)\n",
        "print(f\"Perplexity of New Model on New Data: {perplexity_new_model_on_new_data:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ-W44eEq6I4",
        "outputId": "4916d7ce-f2f3-4719-d34f-80c6f89eb39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Old Model on Old Data: 5.9073\n",
            "Perplexity of New Model on Old Data: 1.3986\n",
            "Perplexity of Old Model on New Data: 5.7196\n",
            "Perplexity of New Model on New Data: 1.4309\n"
          ]
        }
      ]
    }
  ]
}